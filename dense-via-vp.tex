\section{Converting from sparse to dense via Variable Projection}

\subsection{Motivation}

Profiling reveals that for large numbers of visits or objects, jointcal computational performance scales extremely poorly, and is completely dominated by the Cholesky factorization and rank-update steps that we delegate to \texttt{CHOLMOD}.
While there may be some opportunity to gain $<2\times$ speedup improvements by improving the outlier rejection logic and hence requesting fewer rank updates, the fact that most of the time is going into routines we do not control (and moreover are the \emph{only} third-party implementations of the algorithm that supports rank-update that we know of) suggests that the only way to acceptable jointcal computational performance is to replace the sparse Cholesky factorization with something else.

Because Cholesky is generally the fastest \emph{direct} factorization method for symmetric matrices, the other obvious solution in the realm of sparse linear algebra would be to switch to an approximate factorization, such as preconditioned conjugate gradient (PCG).
Utilizing a PCG solver effectively for nonlinear optimization would represent a major change to the jointcal algorithm, however, as one needs to accept the fact that initial steps can be poor early (due to the approximate nature of the solution), and that this is an acceptable tradeoff for the speed of the factorization, but more accuracy is needed for later steps.
The need to identify an effective preconditioner adds another element of tuning and ``black magic`` to these algorithms, and while there is significant prior art using PCG solvers on this kind of problem, adopting that approach would big a larger change to jointcal than we'd ideally like to make.

The alternative proposed here is to switch away from sparse matrices entirely, by ordering the Hessian matrix $H$ such that the vast majority of it is block diagonal.
This allows it to be reduced via a series of small dense subproblems (one per \class{FittedStar}) to yield a moderate-size dense matrix (with dimensionality the number of non-star parameters) that must be factored each iteration.
The approach here is a variant of the Variable Projection algorithm of \citet{1973SJNA...10..413G}, but I will derive it fully from \prettyref{eq:astrometry-chi2} to ensure it is strictly equivalent to the sparse solution.
That is a major advantage of this approach -- while implementing it efficiently in the code may require some restructuring, it should generate (aside from floating point errors, which may of course be substantial) the same sequence of steps as the sparse Cholesky approach.

\subsection{Derivation and Formalism}

We will start by rewriting explicit sums in \prettyref{eq:astrometry-chi2} as matrix products:

\begin{align}
    \chi^{2} & = D^T W D
\end{align}

with

\begin{align}
    D & \equiv \left[
        \begin{array}{ c }
            D_1 \\
            D_2 \\
            \vdots \\
            D_i
        \end{array}
    \right] \\
    W & \equiv \left[
        \begin{array}{ c c c c }
               W_1 & 0 & 0 & 0 \\
            0 &    W_2 & 0 & 0 \\
            0 & 0 & \ddots & 0 \\
            0 & 0 & 0 &    W_i
        \end{array}
    \right]
\end{align}

and each block $D_i$ or $W_i$ corresponds to the residual terms or weights (respectively) for one \class{FittedStar} (including both \class{MeasuredStar} and \class{RefStar} terms, as appropriate).
They thus have the same \emph{content} as the $D$ and $W$ symbols introduced in \prettyref{sec:mathematical-formalism}, but different indexing notation and ordering of elements (the ordering is unspecified and unimportant in \prettyref{sec:mathematical-formalism}; here it is quite important).
Note that the dimensionality of these blocks are not the same for all $i$ -- the size of the blocks for \class{FittedStar} $i$ are set by the number of data points (including reference positions) for that star.

As in the earlier derivation, we recognize that $W$ (and each of its blocks $W_i$) is positive definite, and hence has a square root.
We then define
\begin{align}
    z \equiv W^{1/2}D
\end{align}
and
\begin{align}
    z_i \equiv W_i^{1/2}D_i
\end{align}
to further simplify the $\chi^2$ to just
\begin{align}
    \chi^{2} & = z^Tz
\end{align}

Using this notation, we will now re-derive the solution for a minimization step (analogous to \prettyref{eq:gradient_equation}).
We start with the second-order Taylor expansion of $\chi^2$, again using $\theta$ as the full vector of free parameters and neglecting the second derivatives of the residuals themselves:
\begin{align}
    \chi^2 &\approx z^Tz + 2z^T J\delta\theta
    + \delta\theta^T J^T\!J \delta\theta
\end{align}
with
\begin{align}
    J &\equiv \nabla z.
\end{align}
As with the other symbols, $J$ has the same content as the $J$ from \prettyref{sec:mathematical-formalism}, but different indexing and the order of its rows more explicit.
We wish to zero the derivative of this expansion with respect to $\delta\theta$:
\begin{align}
    \nabla\chi^2 &\approx 2J^T\!z + 2J^T\!J \delta\theta = 0
\end{align}
which yields the equivalent of \prettyref{eq:gradient_equation} in the new notation:
\begin{align}
    J^T\!J \delta\theta = -J^T\!z. \label{eq:vp_gradient_equation}
\end{align}

To proceed, we now have to specify an order for the parameter vector $\theta$ (and columns of $J$):
\begin{itemize}
    \item We start with the positional parameters ($F_i$ in \prettyref{sec:mathematical-formalism}), and, in the future, proper motion and parallax, for each star $i$, in the same order we used for the data.
    For notational consistency and to generalize $F$ (using Greek symbols for free parameters), we will call the delta in these $\mu_i$ and their concatenation $\mu$.
    \item We finish with all other parameters that are not per-star.
    We will discuss an extension that puts any per-visit parameters before any camera constant parameters below, but for now do not need to distinguish between them.
    We will use $\nu$ for these.
\end{itemize}
Or, in block-matrix form,
\begin{align}
    \delta\theta &= \left[
        \begin{array}{ c }
            \mu \\
            \nu
        \end{array}
    \right]
    =
    \left[
        \begin{array}{ c }
            \mu_1 \\
            \mu_2 \\
            \vdots \\
            \mu_i \\
            \nu
        \end{array}
    \right]
\end{align}
The block form of $J$ is then
\begin{align}
    J &= \left[
        \begin{array}{ c c }
            A & B
        \end{array}
    \right]
    = \left[
        \begin{array}{ c c c c c }
            A_1 & 0 & 0 & 0 & B_1 \\
            0 & A_2 & 0 & 0 & B_2 \\
            0 & 0 & \ddots & 0 & \vdots \\
            0 & 0 & 0 & A_i & B_i
        \end{array}
    \right]
\end{align}
where
\begin{align}
    A_i &\equiv \nabla_{\mu_i} z_i \\
    B_i &\equiv \nabla_{\nu} z_i.
\end{align}
The block-diagonal structure of $A$ is what makes this method practical; it comes about because the derivative of the residuals for one star with respect to the parameters of some other star is zero.
Note the number of columns in $A$ (typically $\sim 10^5$) is much larger than the number of columns in $B$ ($\sim 10^3$), but the number of columns in a particular $A_i$ (only 2, or perhaps 5 with stellar motion being fit) is much smaller than the number of columns in a particular $B_i$ (the same as $B$: $\sim 10^3$).

Using the compressed block form, we can expand \prettyref{eq:vp_gradient_equation} as
\begin{align}
    \left[
        \begin{array}{ c }
            A^T \\
            B^T
        \end{array}
    \right]
    \left[
        \begin{array}{ c c }
            A & B
        \end{array}
    \right]
    \left[
        \begin{array}{ c }
            \mu \\
            \nu
        \end{array}
    \right]
    &=
    -\left[
        \begin{array}{ c }
            A^T z \\
            B^T z
        \end{array}
    \right] \\
    A^T A \mu + A^T B \nu &= -A^T z \\
    B^T A \mu + B^T B \nu &= -B^T z
\end{align}
while the expanded version of this system of matrix equations is
\begin{align}
    A_i^T A_i \mu_i + A_i^T B_i \nu &= -A_i^T z_i \label{eq:vp_star_subproblem} \\
    \sum_i B_i^T A_i \mu_i + \left[\sum_i B_i^T B_i\right]\nu &= -\sum_i B_i^T z_i \label{eq:vp_reduced_problem}
\end{align}
This is enough to see the outlines of the algorithm:
\begin{itemize}
    \item for each \class{FittedStar} $i$, solve \prettyref{eq:vp_star_subproblem} for $\mu_i$ \emph{as a function of $\nu$};
    \item substitute the results into \prettyref{eq:vp_reduced_problem} and solve that for $\nu$;
    \item use $\nu$ to fully evaluate each $\mu_i$.
\end{itemize}
The per-\class{FittedStar} subproblems are tiny and dense, while the reduced final problem should usually be small enough that a dense factorization is probably more efficient than a sparse one (especially if using a dense solver makes a parallel algorithm available).

\subsubsection{Simplification via QR Factorization}

The algorithm derived in the last section is the bulk of this proposal, and it should work regardless of the details of the factorization used to solve the subproblems or the reduced problem.
Using a QR factorization of each $A_i$ in the subproblems \prettyref{eq:vp_star_subproblem} simplifies the linear algebra involved in substituting $\mu_i$ back into \prettyref{eq:vp_reduced_problem}, however, so working through that approach seems worthwhile.
It is quite likely that other factorizations would yield something similar.
We will write the column-pivoted QR factorization of $A_i$ aside
\begin{align}
    A_i &= \left[
        \begin{array}{ c c }
            U_i & V_i
        \end{array}
    \right]
    \left[
        \begin{array}{ c }
            R_i \\
            0
        \end{array}
    \right] \\
    P_i^T
    = U_i R_i P_i^T
\end{align}
where $U_i$ and $V_i$ are blocks in an orthogonal matrix:
\begin{align}
    \left[
        \begin{array}{ c c }
            U_i & V_i
        \end{array}
    \right]
    \left[
        \begin{array}{ c }
            U_i^T \\
            V_i^T
        \end{array}
    \right]
    &=
    \left[
        \begin{array}{ c }
            U_i^T \\
            V_i^T
        \end{array}
    \right]
    \left[
        \begin{array}{ c c }
            U_i & V_i
        \end{array}
    \right]
    = I,
\end{align}
$R_i$ is upper triangular, and $P_i$ is a permutation matrix ($R$ and $P$ are unrelated to those defined in \prettyref{subsec:Definitions}; we're just running out of symbols).

Rearranging \prettyref{eq:vp_star_subproblem} and substituting this yields
\begin{align}
    P_i R_i^T R_i P_i^T \mu_i &= -P_i R_i^T U_i^T \left(z_i + B_i\nu\right) \label{eq:vp_star_subproblem_qr_1}
\end{align}

Noting that $\mu_i$ only appears in \prettyref{eq:vp_reduced_problem} as $A_i\mu_i$, we can multiply \prettyref{eq:vp_star_subproblem_qr_1} on the left by $U_i R_i^{-T} P_i^T$ to obtain
\begin{align}
    U_i R_i P_i^T \mu_i &= -U_i U_i^T \left(z_i + B_i\nu\right) \\
    A_i \mu_i = -U_i U_i^T \left(z_i + B_i\nu\right) \label{eq:vp_star_subproblem_qr_2}
\end{align}
which we can the easily insert back into \prettyref{eq:vp_reduced_problem}:
\begin{align}
    -\sum_i B_i^T U_i U_i^T \left(z_i + B_i\nu\right) + \left[\sum_i B_i^T B_i\right]\nu &= -\sum_i B_i^T z_i
\end{align}
and rearrange to obtain
\begin{align}
    \left[\sum_i B_i^T \left(I - U_i U_i^T\right) B_i\right]\nu &=
        -\sum_i B_i^T \left(I - U_i U_i^T\right) z_i \\
    \left[\sum_i B_i^T V_i V_i^T B_i\right]\nu &=
        -\sum_i B_i^T V_i V_i^T z_i \\
    H\nu &= -g.
\end{align}
We can use this to accumulate the reduced Hessian and gradient $H$ and $g$, factor $H$ using the solver of our choice (dense $LDL^T$ seems reasonable), and solve for $\nu$ at every iteration.

While this makes it seem as though we only need $V_i$ from each QR factorization, note that we will need to use $U_i$ and $R_i$ (along with reduced solution $\nu$ for each iteration) to go back and fully solve for the step $\mu_i$ for each \class{FittedStar} as well, via:
\begin{align}
    \mu_i &= - P_i R_i^{-1} U_i^T \left(z_i + B_i\nu\right)
\end{align}
